library(ISLR)
#Wczytanie danych:
data(Hitters)
#Ze wzgledow technicznych wstawiamy zmienna Salary jako ostatnia kolumne:
Hitters=Hitters[,c(1:18,20,19)]
#Usuwamy wiersze z brakami danych:
Hitters=na.omit(Hitters)
#Wykonaujemy PCA na zmiennych objasniajacych:
x=model.matrix(Salary~.,Hitters)[,-1]
x = data.frame(scale(x))
pca = princomp(x)
plot(pca)
dataTemp = data.frame(pca$scores,Hitters$Salary)
names(dataTemp)[ncol(dataTemp)]="Salary"
lm.fit1 = lm(Salary~.,data=dataTemp)
print(summary(lm.fit1))
p = ncol(x)
pca.r2 = numeric(p)
for(j in 1:p){
lm.fit = lm(Salary~.,data=dataTemp[,c(1:j,ncol(dataTemp))])
pca.r2[j] = summary(lm.fit)$r.squared
}
plot(1:p,pca.r2,type="b",col="orange",lwd=2,xlab="Variables",ylab="R2")
#R2 vs najbardziej istotne zmienne z modelu liniowego:
p = ncol(x)
lm.r2 = numeric(p)
lm.fit0 = lm(Salary~.,data=Hitters)
tstat = abs(summary(lm.fit0)$coef[,3])[-1]
order1 = order(tstat,decreasing=T)
for(j in 1:p){
lm.fit = lm(Salary~.,data=Hitters[,c(order1[1:j],ncol(Hitters))])
lm.r2[j] = summary(lm.fit)$r.squared
}
#Wykres R2 vs najbardziej istotne skladowe/zmienne:
plot(1:p,pca.r2,type="b",col="orange",lwd=2,xlab="Variables",ylab="R2",ylim=c(0,0.6))
lines(1:p,lm.r2,col="blue",lwd=2,type="b")
legend("bottomright",c("PCA","LM"),col=c("orange","blue"),lwd=c(2,2),lty=c(1,1))
tstat
summary(lm.fit0)
summary(lm.fit0)$coef
summary(lm.fit)
summary(lm.fit)$r.square
summary(lm.fit)$r.squared
setwd("~/R files/Machine Learning/Laboratoria")
rm(list = ls())
sample.fraction <- function(n, frac) {
return(sample(n, n * frac))
}
# LDA i QDA
# W przypadku lda, zakładam, że rozkład cech wektora objaśniającego ma rozkład wielowymiarowy normalny
# Macierze kowariancji obu klas (tutaj zakladam dla dwoch) są takie same, srednie sa rozne
# Dane Default (metody z pakietu MASS)
library(ISLR)
library(MASS)
# Test wbudowanych metod lda i qda na danych Default
dat <- Default
dat$default <- ifelse(dat$default == "No", 0, 1)
dat$default <- as.factor(dat$default)
dat$student <- ifelse(dat$student == "No", 0, 1)
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
test <- dat[-samp, ]
head(dat)
base.lda <- lda(default ~ ., data = train)
pred.lda <- predict(base.lda, newdata = test[ , -1])
table(pred.lda$class, test$default)
base.qda <- qda(default ~. , data = train)
pred.qda <- predict(base.qda, newdata = test[, -1])
table(pred.qda$class, test$default)
# Własna implementacja lda
# Tutaj zalozylem, ze lda.fit zwraca nam funkcje klasyfikacyjna, wiec w funkcji predict,
# Beda przewidywane klasy na podstwie tej funkcji
lda.fit <- function(train, response) {
train_0 <- train[which(response == 0), ]
train_1 <- train[which(response == 1), ]
# Pstwa apriori
prior0 <- nrow(train_0) / nrow(train)
prior1 <- nrow(train_1) / nrow(train)
# srednie kazdej cechy
means_0 <- sapply(train_0, mean)
means_1 <- sapply(train_1, mean)
# macierze kowariancji i macierz kowariancji wewnatrz grupowej
cov_0 <- cov(train_0)
cov_1 <- cov(train_1)
n0 <- nrow(train_0)
n1 <- nrow(train_1)
W <- ( cov_0 * (n0 - 1) + cov_1 * (n1 - 1) ) / (n0 + n1 - 2)
# Model
model <- function(x) {
a <- t(means_0 - means_1) %*% solve(W) %*% t(as.matrix(x))
b <- -0.5 * t(means_0 - means_1) %*% solve(W) %*% (means_0 + means_1) + log(prior0 / prior1)
fitted_model <- sapply(a, function(x, y) { x + y}, b)
return ( fitted_model)
}
}
predict.lda <- function(model, test) {
f_val <- model(test)
classes <- ifelse(f_val < 0, 1, 0)
return(classes)
}
train_clean <- train[ , -1]
train_lab <- train[ , 1]
test_clean <- test[ , -1]
test_lab <- test[ , 1]
mylda <- lda.fit(train_clean, train_lab)
classes <- predict.lda(mylda, test_clean)
table(classes, test[, 1])
###############################################
############ REGRESJA LOGISTYCZNA #############
###############################################
###### ZADANIE 1 #######
SA <- read.table("https://home.ipipan.waw.pl/p.teisseyre/TEACHING/ZMUM/DANE/SAheart.data", sep = ",", h = T)
# a)
# X = 1 (chd = 1), X = 0 (chd = 0)
# Y = 1 (famhist = present), Y = 0 (famhist = absent)
#      X = 1     X = 0
# Y=1  p11        p10
# Y=0  p01        p00
# OR = (p11p00)/(p01p10)
n <- nrow(SA)
n11 <- sum(SA$chd == 1 & SA$famhist == "Present")
n10 <- sum(SA$chd == 0 & SA$famhist == "Present")
n01 <- sum(SA$chd == 1 & SA$famhist == "Absent")
n00 <- sum(SA$chd == 0 & SA$famhist == "Absent")
# p11 = n11/n
# p10 = n10/n
# p01 = n01/n
# p00 = n00/n
# wyznaczamy iloraz szans dla zmiennych famhist i chd
OR <- (n11*n00)/(n01*n10)
OR
# b)
SA <- SA[,-1]
SA.logit <- glm(chd ~ ., data = SA, family = "binomial")
# c)
summary(SA.logit)
rm(list = ls())
library(ggplot2)
library(ggthemes)
data("USArrests")
head(USArrests)
dat <- data.frame(sapply(USArrests, scale))
rownames(dat) <- rownames(USArrests)
head(dat)
# PCA metodą na piechote
cov_matrix <- cov(dat)
eig <- eigen(cov_matrix)
eig_vectors <- eig$vectors
eig_values <- eig$values
#składowe główne
pc_components1 <- as.matrix(dat) %*% eig_vectors
pc_components1
# Składowe główne wersja R
pc_components2 <- princomp( ~. , data = USArrests, cor = TRUE)
pc_components2$scores
print(summary(pc_components))
vars <- (pc_components2$sdev)^2
vars <- vars / sum(vars)
data = data.frame(vars = vars,
PC = seq(vars))
ggplot(data = data, aes(x = PC, y = vars)) +
geom_point() +
geom_line(group = 1, col = "red") +
theme_test() +
scale_y_continuous(labels = scales::percent)
biplot(pc_components2, choices = 1:2, pc.biplot = TRUE, cex = 0.6)
pc_components2$loadings
library(ISLR)
dat2 <- Hitters
dat2 <- na.omit(dat2)
response <- dat2$Salary
datPCA <- dat2[, -which(colnames(dat2) == "Salary")]
datPCA$League <- ifelse(datPCA$League == 'N', 1, 0)
datPCA$Division <- ifelse(datPCA$Division == 'W', 1, 0)
datPCA$NewLeague <- ifelse(datPCA$NewLeague == 'N', 1, 0)
hitterspca <- princomp( ~ ., data = datPCA, cor = TRUE)
vars <- hitterspca$sdev^2
vars <- vars / sum(vars)
data = data.frame(vars = vars,
PC = seq(vars))
ggplot(data = data, aes(x = PC, y = vars)) +
geom_point() +
geom_line(group = 1, col = "red") +
theme_test() +
scale_y_continuous(labels = scales::percent)
accumulated_var <- cumsum(vars)
data = data.frame(vars = accumulated_var,
PC = seq(accumulated_var))
ggplot(data = data, aes(x = PC, y = vars)) +
geom_point() +
geom_line(group = 1, col = "red") +
theme_gdocs() +
scale_y_continuous(labels = scales::percent)
components <- hitterspca$scores
determination_coef <- function(y_pred, y_real) {
R <- 1 - sum((y_real - y_pred)^2) / sum((y_real - mean(y_real))^2)
return(R)
}
for (i in 1:ncol(components)) {
train <- data.frame(components[, seq(i)])
train$y <- response
model <- lm(y ~ . , data = train)
Rs[i] <- determination_coef(model$fitted.values, train$y)
}
Rs <- numeric(ncol(components))
for (i in 1:ncol(components)) {
train <- data.frame(components[, seq(i)])
train$y <- response
model <- lm(y ~ . , data = train)
Rs[i] <- determination_coef(model$fitted.values, train$y)
}
plot(Rs, col ='red')
plot(Rs, col ='red')
train <- data.frame(datPCA)
train$y <- response
model <- lm(y ~. , data = train)
s <- summary(model)
variables <- rownames(s$coefficients)
variables <- variables[order(s$coefficients[ , "Pr(>|t|)"], decreasing = TRUE)]
variables <- variables[variables != '(Intercept)']
Rs2 <- numeric(length(variables))
for (i in 1:ncol(datPCA)) {
train <- data.frame(datPCA[, variables[seq(i)]])
train$y <- response
model <- lm(y ~ . , data = train)
Rs2[i] <- determination_coef(model$fitted.values, train$y)
}
plot(Rs, col ='red')
points(Rs2, col ='blue')
rm(list =ls())
# used libraries
library(rpart)
library(rpart.plot)
library(ggplot2)
library(dplyr)
# Zadanie nr 1
setwd("C:/Users/piotr/Desktop/Github- projects/Machine-Learning/Laboratoria ZMUM/DecisionTrees")
data <- read.table('../data/SAheart.data', sep = ',', header = TRUE, row.names = 1)
head(data)
samp <- sample(nrow(data), 0.8 *nrow(data))
train <- data[samp, ]
test <- data[-samp, ]
tree <- rpart(formula = chd ~ .,
data = train,
control = rpart.control(minsplit = 5, cp = 0.01))
rpart.plot(tree)
pred <- predict(tree, newdata =  test[, -ncol(test)])
classes <- ifelse(pred > 0.5, 1, 0)
t <- table(test$chd, classes)
acc <- (t[1, 1] + t[2 , 2]) / sum(t)
acc
head(data)
dat <- dropCols(train, c('famhist', 'chd'))
f <- mostFreq(train$famhist)
f$value
mean_record <- sapply(dat , mean)
mean_record <- cbind(data.frame(t(mean_record)), famhist = f$value)
predict(tree, mean_record)
source('../Functions/functions.R')
dat <- dropCols(train, c('famhist', 'chd'))
f <- mostFreq(train$famhist)
f$value
mean_record <- sapply(dat , mean)
mean_record <- cbind(data.frame(t(mean_record)), famhist = f$value)
predict(tree, mean_record)
plotcp(tree)
tree$cptable
z <- prune.rpart(tree, cp=0.038)
rpart.plot(z)
plotcp(tree)
z <- prune.rpart(tree, cp=0.038)
rpart.plot(z)
plotcp(tree)
data <- read.table('../data/SAheart.data', sep = ',', header = TRUE, row.names = 1)
head(data)
samp <- sample(nrow(data), 0.8 *nrow(data))
train <- data[samp, ]
test <- data[-samp, ]
tree <- rpart(formula = chd ~ .,
data = train,
control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(tree)
pred <- predict(tree, newdata =  test[, -ncol(test)])
classes <- ifelse(pred > 0.5, 1, 0)
t <- table(test$chd, classes)
acc <- (t[1, 1] + t[2 , 2]) / sum(t)
acc
head(data)
dat <- dropCols(train, c('famhist', 'chd'))
f <- mostFreq(train$famhist)
f$value
mean_record <- sapply(dat , mean)
mean_record <- cbind(data.frame(t(mean_record)), famhist = f$value)
predict(tree, mean_record)
plotcp(tree)
Ft <- read.table("https://home.ipipan.waw.pl/p.teisseyre/TEACHING/ZMUM/DANE/fitness.txt", h = T)
# a) - dopasujemy drzewo regresyjne na podstawie wszystkich atrybutow
set.seed(123456)
Ft.tree <- rpart(Oxygen ~ ., data = Ft, cp = 0.01, minsplit = 2)
pdf(file="tree1.pdf")
rpart.plot(Ft.tree)
dev.off()
# graficzna reprezentacja drzewa
par(mar = c(0, 0, 0, 0))
plot(Ft.tree)
text(Ft.tree)
par(mar = c(4, 4, 4, 4))
# b) - dla jakiego biegacza pobor tlenu uznajemy za najwiekszy?
# ??????????????????????
# c) - dokonaj prognozy poboru tlenu dla obserwacji, ktorej wartosci
#      zmiennych objasniajacych sa rowne medianom ze zbioru Ft
x0 <- apply(Ft[,-3], 2, median)
print(x0)
predict(Ft.tree, data.frame(t(x0)))
# d) - przytnijmy drzewo stosujac regule kosztu-zlozonosci oraz regule 1SE
plotcp(Ft.tree)
rm(list  = ls())
source('../Functions/functions.R')
fitness <- read.table('../data/fitness.txt', header = TRUE)
# Dopasowuje drzewo regresyjne do danych
regression.tree <- rpart(formula = Oxygen ~ . ,
data  = fitness,
control = rpart.control(minsplit = 2, cp = 0.01))
# Wykres drzewa
rpart.plot(regression.tree)
# Najwiekszy pobor tlenu dla biegacza, ktory ma najkrotszy czas
# Rekord testowy
medians <- data.frame(t(sapply(dropCols(fitness, 'Oxygen'), median)))
medians
pred <- predict(regression.tree, medians)
pred
# Zgadza sie z drzewem
# Podpunt d
plotcp(regression.tree)
