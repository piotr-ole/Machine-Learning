# ??????????????????????
# c) - dokonaj prognozy poboru tlenu dla obserwacji, ktorej wartosci
#      zmiennych objasniajacych sa rowne medianom ze zbioru Ft
x0 <- apply(Ft[,-3], 2, median)
print(x0)
predict(Ft.tree, data.frame(t(x0)))
# d) - przytnijmy drzewo stosujac regule kosztu-zlozonosci oraz regule 1SE
plotcp(Ft.tree)
printcp(Ft.tree)
rm(list  = ls())
source('../Functions/functions.R')
fitness <- read.table('../data/fitness.txt', header = TRUE)
# Dopasowuje drzewo regresyjne do danych
regression.tree <- rpart(formula = Oxygen ~ . ,
data  = fitness,
control = rpart.control(minsplit = 2, cp = 0.01))
# Wykres drzewa
rpart.plot(regression.tree)
# Najwiekszy pobor tlenu dla biegacza, ktory ma najkrotszy czas
# Rekord testowy
medians <- data.frame(t(sapply(dropCols(fitness, 'Oxygen'), median)))
medians
pred <- predict(regression.tree, medians)
pred
# Zgadza sie z drzewem
# Podpunt d
plotcp(regression.tree)
printcp(regression.tree)
regression.tree$cptable
plotcp(regression.tree)
printcp(regression.tree)
t <- prune.rpart(regression.tree, cp = 0.48)
rpart.plot(t)
t <- prune.rpart(regression.tree, cp = 0.048)
rpart.plot(t)
plotcp(regression.tree)
# podpunkt e
rm(list = ls())
tree.reg <- rpart(Oxygen ~ Runtime + Age,
data = fitness,
control = rpart.control(minsplit = 2, cp = 0.02))
source('../Functions/functions.R')
fitness <- read.table('../data/fitness.txt', header = TRUE)
tree.reg <- rpart(Oxygen ~ Runtime + Age,
data = fitness,
control = rpart.control(minsplit = 2, cp = 0.02))
head(fitness)
tree.reg <- rpart(Oxygen ~ RunTime + Age,
data = fitness,
control = rpart.control(minsplit = 2, cp = 0.02))
rpart.plot(tree.reg)
library(plotly)
tree.grid = expand.grid(
RunTime = seq(min(fitness$RunTime), max(fitness$RunTime), length = 50),
Age = seq(min(fitness$Age), max(fitness$Age), length = 50)
)
pred = predict(tree.reg, newdata = tree.grid)
head(pred)
Ft.tree1 <- rpart(Oxygen ~ ., data = Ft[,c(1, 3, 4)], cp = 0.02, minsplit = 2)
n_point <- 100
Age1 <- seq(from = 35, to = 60, length.out = n_point)
RunTime1 <- seq(from = 8, to = 15, length.out = n_point)
newdata <- expand.grid(Age = Age1, RunTime = RunTime1)
newdata$Z <- predict(Ft.tree1, newdata = newdata)
###################################################################
###############            ZADANIE 1           ####################
###################################################################
library("rpart")
library("rpart.plot")
SA <- read.table("https://home.ipipan.waw.pl/p.teisseyre/TEACHING/ZMUM/DANE/SAheart.data", h = T, row.names = 1, sep = ",")
# a) - dopasujemy drzewo klasyfikacyjne z parametrami cp = 0.01 i minsplit = 5
# wazne, aby ustawic as.factor(chd) by zbudowac drzewo klasyfikacyjne a nie regresyjne
tree <- rpart(as.factor(chd) ~ ., data = SA, cp = 0.001, minsplit = 5)
pdf(file="tree.pdf")
rpart.plot(tree)
dev.off()
print(summary(tree), digits = 4)
# Opis drzewa:
#1 - numer wezla
#2 - Nazwa zmiennej wg, ktorej dokonano podzialu i warunek realizacji podzialu
#3 - Liczba elementow w wezle
#4 - Liczba elementow blednie zaklasyfikowanych
#5 - Predykcja przynaleznosci klasowej
#6 - wektor estymatorow pst. przynaleznosci klasowej
#    (nazwy uporzadkowane leksograficznie)
# b) - graficzna reprezentacja drzewa
par(mar = c(0, 1, 0, 1))
plot(tree)
text(tree, use.n = TRUE)
par(mar = c(4, 4, 4, 4))
# c) - dokonujemy predykcji klasy dla obserwacji ze zmiennymi objasniajacymi bedacymi srednimi
#      wartosciami zmiennych ze zbioru SA
x0 <- as.data.frame(t(apply(SA[,-c(5)], 2, mean)))
table(SA$famhist)
# Absent Present
# 270     192
x0 <- cbind(x0, famhist = "Absent")
predict(tree, newdata = rbind(x0), type = "class")
# d) - wybierzmy teraz drzewo optymalne w oparciu o regule 1SE
plotcp(tree)
# rysunek wykonany na podstawie tabeli cptable
tree$cptable
# przycinamy drzewo wybierajac parametr cp zgodnie z regula 1SE
Z <- prune.rpart(tree, cp = 0.04)
rpart.plot(tree)
rpart.plot(Z)
par(mar = c(0, 1, 0, 1))
plot(Z)
text(Z, use.n = TRUE)
par(mar = c(4, 4, 4, 4))
# wybor innego kryterium podzialu (miary roznorodnosci) mozna dokonac poprzez
# ustawienie listy parametrow
tree.info <- rpart(as.factor(chd) ~ ., data = SA, cp = 0.01, minsplit = 5,
parms = list(split = "information"))
par(mar = c(0, 1, 0, 1))
plot(tree.info)
text(tree.info, use.n = TRUE)
par(mar = c(4, 4, 4, 4))
par(mar = c(0, 0, 0, 0))
par(mfrow = c(1, 2))
plot(tree)
text(tree, use.n = TRUE)
plot(tree.info)
text(tree.info, use.n = TRUE)
par(mfrow = c(1, 1))
par(mar = c(4, 4, 4, 4))
###################################################################
###############            ZADANIE 2           ####################
###################################################################
library(ggplot2)
Dane <- read.table("https://home.ipipan.waw.pl/p.teisseyre/TEACHING/ZMUM/DANE/earthquake.txt", h = T)
# a) - tworzymy wykres rozproszenia
q <- ggplot(data = Dane, aes(x = body, y = surface, col = popn))+
geom_point(size = 2.5) +
theme(axis.title.x = element_text(size = 15, angle = 0, face = "italic"),
axis.title.y = element_text(size = 15, angle = 90, face = "italic")) +
theme(legend.title = element_text(size = 14, face = "bold")) +
theme(legend.text = element_text(colour = "black", size = 12)) +
guides(color = guide_legend(keywidth = 2, keyheight = 2))
q
# b) - graficzna reprezentacja drzewa
# DRZEWO minsplit = 5:
Dane.tree <- rpart(as.factor(popn) ~ ., data = Dane, minsplit = 5) # dopasowujemy drzewo
par(mar = c(0, 0, 0, 0))
plot(Dane.tree)
text(Dane.tree)
par(mar = c(4, 4, 4, 4))
# tworzymy pomocnicze wartosci dla zmiennych objasniajacych
xp <- seq(4.5, 6.5, length = 50)
yp <- seq(3.5, 6.5, length = 50)
siatka <- expand.grid(body = xp, surface = yp)
# dokonujemy predykcji na sztucznie stworzonych obserwacjach ze zbioru siatka
P1 <- predict(Dane.tree, newdata = siatka, type = "prob")
zp1 <- ifelse(P1[,1] == 1, 1, -1)
# tworzymy wykres konturowy rozdzielajacy klasy wedlug reguly opartej o drzewo klasyfikacyjne
n1 <- 20
n2 <- 9
kol <- c(rep("black", n1), rep("red", n2))
sym <- c(rep("Q", n1), rep("X", n2))
plot(Dane$body, Dane$surface, type = "n", xlab = "body", ylab = "surface")
text(Dane$body, Dane$surface, sym, col = kol)
contour(xp, yp, matrix(zp1, 50), level = 0, add = T)
# alternatywnie przy pomocy ggplot2
q <- ggplot() + xlim(c(4.4, 6.6)) + ylim(c(3.4, 6.8)) +
geom_contour(data = cbind(siatka, data.frame(pred2 = zp1)),
aes(x = body, y = surface, z = pred2), bins = 1, col = "black",
linetype = 1, size = 1.2, alpha = 0.5) +
geom_point(data = Dane, aes(x = body, y = surface, col = popn), size = 2) +
xlab("body") + ylab("surface") +
theme(plot.title = element_text(lineheight  = .8, face = "bold", hjust = 0.5)) +
theme(axis.title.x = element_text(size = 15, angle = 0, face = "italic"),
axis.title.y = element_text(size = 15, angle = 90, face = "italic")) +
theme(legend.title = element_text(size = 14, face = "bold")) +
theme(legend.text = element_text(colour = "black", size = 12)) +
guides(color = guide_legend(keywidth = 2, keyheight = 2))
q
# DRZEWO minsplit = 15:
Dane.tree <- rpart(as.factor(popn) ~ ., data = Dane, minsplit = 15)
par(mar = c(0, 0, 0, 0))
plot(Dane.tree)
text(Dane.tree)
par(mar = c(4, 4, 4, 4))
P2 <- predict(Dane.tree, newdata = siatka, type = "prob")
zp2 <- ifelse(P2[, 1] == 1, 1, -1)
n1 <- 20
n2 <- 9
kol <- c(rep("black", n1), rep("red", n2))
sym <- c(rep("Q", n1), rep("X", n2))
plot(Dane$body, Dane$surface, type = "n", xlab = "body", ylab = "surface")
text(Dane$body, Dane$surface, sym, col = kol)
contour(xp, yp, matrix(zp2, 50), level = 0, add = T)
# alternatywnie przy pomocy ggplot2
q <- ggplot() + xlim(c(4.5, 6.6)) + ylim(c(3.5, 6.5)) +
geom_contour(data = cbind(siatka, data.frame(pred2 = zp2)),
aes(x = body, y = surface, z = pred2), bins = 1, col = "black",
linetype = 1, size = 1.2, alpha = 0.5) +
geom_point(data = Dane, aes(x = body, y = surface, col = popn), size = 2) +
xlab("body") + ylab("surface") +
theme(plot.title = element_text(lineheight  = .8, face = "bold", hjust = 0.5)) +
theme(axis.title.x = element_text(size = 15, angle = 0, face = "italic"),
axis.title.y = element_text(size = 15, angle = 90, face = "italic")) +
theme(legend.title = element_text(size = 14, face = "bold")) +
theme(legend.text = element_text(colour = "black", size = 12)) +
guides(color = guide_legend(keywidth = 2, keyheight = 2))
q
###################################################################
###############            ZADANIE 3           ####################
###################################################################
library(rpart)
Ft <- read.table("https://home.ipipan.waw.pl/p.teisseyre/TEACHING/ZMUM/DANE/fitness.txt", h = T)
# a) - dopasujemy drzewo regresyjne na podstawie wszystkich atrybutow
set.seed(123456)
Ft.tree <- rpart(Oxygen ~ ., data = Ft, cp = 0.01, minsplit = 2)
pdf(file="tree1.pdf")
rpart.plot(Ft.tree)
dev.off()
# graficzna reprezentacja drzewa
par(mar = c(0, 0, 0, 0))
plot(Ft.tree)
text(Ft.tree)
par(mar = c(4, 4, 4, 4))
# b) - dla jakiego biegacza pobor tlenu uznajemy za najwiekszy?
# ??????????????????????
# c) - dokonaj prognozy poboru tlenu dla obserwacji, ktorej wartosci
#      zmiennych objasniajacych sa rowne medianom ze zbioru Ft
x0 <- apply(Ft[,-3], 2, median)
print(x0)
predict(Ft.tree, data.frame(t(x0)))
# d) - przytnijmy drzewo stosujac regule kosztu-zlozonosci oraz regule 1SE
plotcp(Ft.tree)
printcp(Ft.tree)
# regula kosztu-zlozonosci
Ft.tree.prune <- prune.rpart(Ft.tree, cp = 0.048)
par(mar = c(0, 0, 0, 0))
plot(Ft.tree.prune)
text(Ft.tree.prune)
par(mar = c(4, 4, 4, 4))
# regula 1SE
Ft.tree.prune <- prune.rpart(Ft.tree, cp = 0.14)
par(mar = c(0, 0, 0, 0))
plot(Ft.tree.prune)
text(Ft.tree.prune)
par(mar = c(4, 4, 4, 4))
rpart.plot(tree.reg, digits = 4)
head(pred)
dat <- cbind(tree.grid, pred)
head(dat)
?plotly
?plot_ly
plot_ly(data = dat, x = Runtime, y = Age, z = pred)
plot_ly(data = dat, x = RunTime, y = Age, z = pred)
head(dat)
plot_ly(x = dat$RunTime, y = dat$Age, z = dat$pred)
?plot_ly
plot_ly(dat, x = ~RunTime, y = ~Age, z = ~pred)
plot_ly(dat, x = ~RunTime, y = ~Age, z = ~pred) %>% add_surface()
volcano
plot_ly(x = dat$RunTime, y = dat$Age, z = dat$pred) %>% add_surface()
kd <- with(MASS::geyser, MASS::kde2d(duration, waiting, n = 50))
kd
plot_ly(x = dat$RunTime, y = dat$Age, z = matrix(dat$pred, nrow = nrow(dat), ncol = ncol(dat))) %>% add_surface()
kd <- with(MASS::geyser, MASS::kde2d(duration, waiting, n = 50))
p <- plot_ly(x = kd$x, y = kd$y, z = kd$z) %>% add_surface()
p
?persp
persp(x = dat$RunTime, y = dat$Age, z = dat$pred)
library(ISLR)
setwd("C:/Users/piotr/Desktop/Github- projects/Machine-Learning/Laboratoria ZMUM/EvaluationMeasures")
rm(list = ls())
library(ISLR)
dat <- Default
head(dat)
train <- sample_frac(dat, 0.8 * nrow(dat))
train <- sample_frac(dat, 0.8)
?sample_frac
test <- dat[!(dat %in% train)]
test <- dat[!(dat %in% train), ]
testTrainSet <- function(dat, frac = 0.8) {
samp <- sample(nrow(dat), frac * nrow(dat))
return(list( train = dat[samp, ], test = dat[-samp, ]))
}
source('../Functions/functions.R')
dat <- Default
head(dat)
trainTestSet <- function(dat, frac = 0.8) {
samp <- sample(nrow(dat), frac * nrow(dat))
return(list( train = dat[samp, ], test = dat[-samp, ]))
}
train, test <- trainTestSet(dat)
list(train, test) <- trainTestSet(dat)
c(train, test) <- trainTestSet(dat)
(train, test) <- trainTestSet(dat)
set <- trainTestSet(dat)
set <- trainTestSet(dat, 0.5)
model.glm <- glm(data = set$train, formula = default ~.)
model.glm <- glm(default ~. , data = set$train)
class(dat$student)
dat[, 'default']
dat[, c('default', 'student')]
dat[, -c('default', 'student')]
?whichc
?which
numerize <- function(dat, colnames) {
for (name in colnames){
values <- levels(as.factor(dat[, name]))
mapped <- seq(0, len(values))
dat[ , name] <- sappy(dat[ , name], function(x, vals, maps) { ind <- which(vals == x); return(maps[ind]) },
values, mapped)
}
return(dat)
}
dat
dat <- numerize(dat, c('default', 'student'))
numerize <- function(dat, colnames) {
for (name in colnames){
values <- levels(as.factor(dat[, name]))
mapped <- seq(0, length(values))
dat[ , name] <- sappy(dat[ , name], function(x, vals, maps) { ind <- which(vals == x); return(maps[ind]) },
values, mapped)
}
return(dat)
}
dat <- numerize(dat, c('default', 'student'))
numerize <- function(dat, colnames) {
for (name in colnames){
values <- levels(as.factor(dat[, name]))
mapped <- seq(0, length(values))
dat[ , name] <- sapply(dat[ , name], function(x, vals, maps) { ind <- which(vals == x); return(maps[ind]) },
values, mapped)
}
return(dat)
}
dat <- numerize(dat, c('default', 'student'))
head(dat)
rm(list = ls())
source('../Functions/functions.R')
dat <- Default
head(dat)
dat <- numerize(dat, c('default', 'student'))
set <- trainTestSet(dat, 0.5)
model.glm <- glm(default ~. , data = set$train)
model.rpart <- rpart(default ~., data = set$train,
control = rpart.control(minsplit = 2, cp = 0.01))
library(rpart.plot)
rpart.plot(model.rpart)
head(set$train)
head(data)
head(dat)
model.rpart <- rpart(default ~., data = set$train,
control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(model.rpart)
accuracy <- function(model, testSet) {
pred = predict(model, newdata = testSet)
t <- table(pred$classes, testSet$default)
}
accuracy(model.glm, set$test)
pred = predict(model.glm, newdata = set$test)
pred = predict(model.glm, newdata = set$test, type = 'response')
pred = predict(model.glm, newdata = set$test, type = 'prob')
pred = predict(model.glm, newdata = set$test, type = 'response')
pred = predict(model.glm, newdata = set$test[-1, ], type = 'response')
pred = predict(model.glm, newdata = set$test[, -1 ], type = 'response')
pred
model.glm <- glm(default ~. , data = set$train, family = 'binomial')
pred = predict(model.glm, newdata = set$test)
pred = predict(model.glm, newdata = set$test, type = 'response')
?glm
dat <- Default
head(dat)
dat <- numerize(dat, c('default', 'student'))
set <- trainTestSet(dat, 0.5)
model.glm <- glm(default ~. , data = set$train, family = 'binomial')
pred = predict(model.glm, newdata = set$test, type = 'response')
pred
head(set$test)
head(Default)
cl <- ifelse(pred > 0.5, 1 , 0)
table(cl, set$test$default)
metrics <- function(model, testSet, class_index) {
pred = predict(model, newdata = testSet, type = 'response')
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 1])
precision <- t[2, 2] / sum(t[1 , ])
return(list(accuracy = acc, recall = recall, precision = precision))
}
l <- metrics(model.glm, set$test, 1)
l$accuracy
l$recall
l$precision
print(t)
metrics <- function(model, testSet, class_index) {
pred = predict(model, newdata = testSet, type = 'response')
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 1])
precision <- t[2, 2] / sum(t[1 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
l <- metrics(model.glm, set$test, 1)
metrics <- function(model, testSet, class_index) {
pred = predict(model, newdata = testSet, type = 'response')
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
l <- metrics(model.glm, set$test, 1)
l$accuracy
l$recall
l$precision
t <- table(predicted, true = testSet[, class_index])
metrics <- function(model, testSet, class_index) {
pred = predict(model, newdata = testSet, type = 'response')
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, true = testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
l <- metrics(model.glm, set$test, 1)
model.glm <- glm(default ~. , data = set$train, family = 'binomial')
model.rpart <- rpart(default ~., data = set$train,
control = rpart.control(minsplit = 2, cp = 0.01))
glm.metrics <- metrics(model.glm, set$test, 1)
rpart.metrics <- metrics(model.rpart, set$test, 1)
metrics <- function(model, testSet, class_index, type) {
pred = predict(model, newdata = testSet, type = type)
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, true = testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
glm.metrics <- metrics(model.glm, set$test, 1, type = 'response')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'prob')
predict(model.rpart, newdata = set$test, type = 'prob')
model.rpart <- rpart(default ~., data = set$train,
control = rpart.control(minsplit = 2, cp = 0.01))
predict(model.rpart, newdata = set$test)
model.rpart <- rpart(as.factor(default) ~., data = set$train,
control = rpart.control(minsplit = 2, cp = 0.01))
rpart.plot(model.rpart)
glm.metrics <- metrics(model.glm, set$test, 1, type = 'response')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'prob')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'response')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'prob')
predict(model.rpart, newdata = set$test, type = 'prob')
metrics <- function(model, testSet, class_index, type) {
if (type == 'glm') {
pred = predict(model, newdata = testSet, type = 'response')
}
else if (type == 'rpart') {
pred = predict(model, newdata = testSet, type = 'prob')[2]
}
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, true = testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
glm.metrics <- metrics(model.glm, set$test, 1, type = 'glm')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'rpart')
predict(model.rpart, newdata = set$test, 1 , type = 'prob')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'rpart')
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'rpart')
metrics <- function(model, testSet, class_index, type) {
if (type == 'glm') {
pred = predict(model, newdata = testSet, type = 'response')
}
else if (type == 'rpart') {
pred = predict(model, newdata = testSet, type = 'prob')[2]
}
predicted <- ifelse(pred > 0.5, 1 , 0)
print(predicted)
t <- table(predicted, true = testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'rpart')
metrics <- function(model, testSet, class_index, type) {
if (type == 'glm') {
pred = predict(model, newdata = testSet, type = 'response')
}
else if (type == 'rpart') {
pred = predict(model, newdata = testSet, type = 'prob')[, 2]
}
predicted <- ifelse(pred > 0.5, 1 , 0)
t <- table(predicted, true = testSet[, class_index])
acc <- sum(diag(t)) / sum(t)
recall <- t[2, 2] / sum(t[ , 2])
precision <- t[2, 2] / sum(t[2 , ])
print(t)
return(list(accuracy = acc, recall = recall, precision = precision))
}
rpart.metrics <- metrics(model.rpart, set$test, 1, type = 'rpart')
glm.metrics <- metrics(model.glm, set$test, 1, type = 'glm')
