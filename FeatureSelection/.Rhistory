str(dat_labels)
dat_labels <- vector(dat_labels)
dat_labels <- dat_labels$V1
dat_labels <- as.factor(dat_labels$V1)
dat_labels <- as.factor(dat_labels)
descrCor <-  cor(dat)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .95)
highlyCorDescr
dat <- dat[-highlyCorDescr]
filterCtrl <- sbfControl(functions = rfSBF, method = "repeatedcv", repeats = 5)
set.seed(10)
rfWithFilter <- sbf(dat, as.factor(dat_labels), sbfControl = filterCtrl)
rfWithFilter
filterCtrl <- sbfControl(functions = AdaBag, method = "repeatedcv", repeats = 5)
filterCtrl <- sbfControl(functions = rqlasso, method = "repeatedcv", repeats = 5)
?sbfControl
filterCtrl <- sbfControl(functions = AdaSBF, method = "repeatedcv", repeats = 5)
filterCtrl <- sbfControl(functions = treebagSBF, method = "repeatedcv", repeats = 5)
set.seed(10)
rfWithFilter <- sbf(dat, as.factor(dat_labels), sbfControl = filterCtrl)
rfWithFilter
filterCtrl <- sbfControl(functions = caretSBF, method = "repeatedcv", repeats = 5)
set.seed(10)
rfWithFilter <- sbf(dat, as.factor(dat_labels), sbfControl = filterCtrl)
rfWithFilter
library(mlbench)
library(Hmisc)
library(randomForest)
install.packages("Hmisc")
install.packages("mlbench")
library(mlbench)
library(Hmisc)
subsets <- c(1:5, 10, 15, 20, 25)
ctrl <- rfeControl(functions = lmFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
set.seed(10)
ctrl <- rfeControl(functions = lmFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(dat, dat_labels,
sizes = subsets,
rfeControl = ctrl)
lmProfile
dat <- read.table('artificial_train.data')
dat_labels <- read.table('artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels <- as.factor(dat_labels)
descrCor <-  cor(dat)
highlyCorDescr <- findCorrelation(descrCor, cutoff = .95)
dat <- dat[-highlyCorDescr]
subsets <- c(1:5, 10, 15, 20, 25)
set.seed(10)
ctrl <- rfeControl(functions = lmFuncs,
method = "repeatedcv",
repeats = 5,
verbose = FALSE)
lmProfile <- rfe(dat, dat_labels,
sizes = subsets,
rfeControl = ctrl)
library(caret)
library(e1071)
library(klaR)
library(mlbench)
library(Hmisc)
library(randomForest)
library(devtools)
library(bounceR)
library(xgboost)
rm(list = ls())
setwd("~/R files/Machine Learning/Projekt 2 - ZMUM")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
p_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( p_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( p_xgb > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: \n", bcs_glm[i]))
}
print(paste0("Balanced [xgb] accuraccy after ", cv_num, " fold CV: ", mean(bcs_xgb)))
print(paste0("Balanced [glm] accuraccy after ", cv_num, " fold CV: ", mean(bcs_glm)))
library(caret)
library(e1071)
library(klaR)
library(mlbench)
library(Hmisc)
library(randomForest)
library(devtools)
library(bounceR)
library(xgboost)
install.packages("e1071")
install.packages("klaR")
install.packages("mlbench")
ddevtools::install_github("STATWORX/bounceR")
devtools::install_github("STATWORX/bounceR")
library(bounceR)
devtools::install_github("STATWORX/bounceR")
install.packages("dplyr")
install.packages("dplyr")
install.packages("purrr")
devtools::install_github("STATWORX/bounceR")
library(bounceR)
rm(list = ls())
library(caret)
library(e1071)
library(klaR)
library(mlbench)
library(Hmisc)
library(randomForest)
library(devtools)
library(bounceR)
library(xgboost)
rm(list = ls())
setwd("~/R files/Machine Learning/Projekt 2 - ZMUM")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
p_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( p_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( p_xgb > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: \n", bcs_glm[i]))
}
balanced_acc(predicted_classes_xgb, test_lab)
balanced_acc(predicted_classes_glm, test_lab)
p_xgb
p_xgb[1:10]
p_glm[1:10]
pred_glm[1:10]
rm(list = ls())
setwd("~/R files/Machine Learning/Projekt 2 - ZMUM")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
p_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( p_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( p_glm > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: ", bcs_glm[i]))
}
setwd("~/Studia IAD/Projekt 2 ZMUM/Machine-Learning/FeatureSelection")
rm(list = ls())
rm(list = ls())
setwd("~/Studia IAD/Projekt 2 ZMUM/Machine-Learning/FeatureSelection")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
p_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( p_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( p_glm > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: ", bcs_glm[i]))
}
print(paste0("Balanced [xgb] accuraccy after ", cv_num, " fold CV: ", mean(bcs_xgb)))
print(paste0("Balanced [glm] accuraccy after ", cv_num, " fold CV: ", mean(bcs_glm)))
rm(list = ls())
setwd("~/Studia IAD/Projekt 2 ZMUM/Machine-Learning/FeatureSelection")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
pred_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( ped_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( pred_glm > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: ", bcs_glm[i]))
}
print(paste0("Balanced [xgb] accuraccy after ", cv_num, " fold CV: ", mean(bcs_xgb)))
print(paste0("Balanced [glm] accuraccy after ", cv_num, " fold CV: ", mean(bcs_glm)))
rm(list = ls())
setwd("~/Studia IAD/Projekt 2 ZMUM/Machine-Learning/FeatureSelection")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering
features_names <- featureFiltering(data = cbind(y = train_lab, train),
target = "y",
method = "cc",
returning = "names")
indices <- integer(length(features_names))
for (j in seq(length(features_names))) {
indices[j] <- which(colnames(train) == features_names[i])
}
indices <- sort(indices)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
pred_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( pred_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( pred_glm > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: ", bcs_glm[i]))
}
print(paste0("Balanced [xgb] accuraccy after ", cv_num, " fold CV: ", mean(bcs_xgb)))
print(paste0("Balanced [glm] accuraccy after ", cv_num, " fold CV: ", mean(bcs_glm)))
?step
install.packages("FSelector")
library(FSelector)
?information.gain
data(iris)
weights <- information.gain(Species~., iris)
print(weights)
subset <- cutoff.k(weights, 2)
f <- as.simple.formula(subset, "Species")
print(f)
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
df <- data.frame(cbind(dat, y = dat_labels))
head(df)
weights <- information.gain(y ~ ., df)
print(weights)
df
subset <- cutoff.k(weights, 2)
subset
f <- as.simple.formula(subset, "Species")
f <- as.simple.formula(subset, "y")
print(f)
weights > 0
weights[weights > 0]
subset <- cutoff.k(weights, 6)
rm(list = ls())
setwd("~/Studia IAD/Projekt 2 ZMUM/Machine-Learning/FeatureSelection")
source("functions.R")
## wersja podstawowa bez selekcji cech
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
thresh = 0.5
cv_num <- 5
bcs_xgb <- numeric(cv_num)
bcs_glm <- numeric(cv_num)
for (i in seq(cv_num)) {
samp <- sample.fraction(nrow(dat), 0.8)
train <- dat[samp, ]
train_lab <- dat_labels[samp]
test <- dat[-samp, ]
test_lab <- dat_labels[-samp]
train <- normalization(train)
test <- normalization(test)
# feature selection
## filtering in bouncer
#
# features_names <- featureFiltering(data = cbind(y = train_lab, train),
#                             target = "y",
#                             method = "cc",
#                             returning = "names")
#
# indices <- integer(length(features_names))
# for (j in seq(length(features_names))) {
#   indices[j] <- which(colnames(train) == features_names[i])
# }
# indices <- sort(indices)
# information gain
weights <- information.gain(y ~ ., data.frame(cbind(train, y = train_lab)))
print(weights)
subset <- cutoff.k(weights, 6)
train <- train[ , -indices]
test <- test[ , -indices]
# fitting model
fit_xgb <-  xgboost(data = data.matrix(train), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.4)
d <- data.frame(train)
d$y <- as.factor(train_lab)
fit_glm <- glm(data = d, formula = y ~ . , family = "binomial")
# prediction
pred_xgb <-  predict(fit_xgb, newdata = data.matrix(test))
predicted_classes_xgb <- ifelse( pred_xgb > thresh, 1, 0)
bcs_xgb[i] <- balanced_acc(predicted_classes_xgb, test_lab)
print(paste0("Balanced accuraccy (xgb) after ", i , "/", cv_num, " fold CV: ", bcs_xgb[i]))
pred_glm <- predict(fit_glm, newdata = data.frame(test), type = "response")
predicted_classes_glm <- ifelse( pred_glm > thresh, 1, 0)
bcs_glm[i] <- balanced_acc(predicted_classes_glm, test_lab)
print(paste0("Balanced accuraccy (glm) after ", i , "/", cv_num, " fold CV: ", bcs_glm[i]))
}
print(paste0("Balanced [xgb] accuraccy after ", cv_num, " fold CV: ", mean(bcs_xgb)))
print(paste0("Balanced [glm] accuraccy after ", cv_num, " fold CV: ", mean(bcs_glm)))
weights <- information.gain(y ~ ., data.frame(cbind(train, y = train_lab)))
subset <- cutoff.k(weights, 6)
subset
train[subset]
indices <- indices_fun(train, subset)
indices_fun <- function(dat, features_names)
{
len <- length(features_names)
indices <- numeric(len)
for (j in seq(len)) {
indices[j] <- which(colnames(dat) == features_names[i])
}
return(indices)
}
indices <- indices_fun(train, subset)
train[, subset]
