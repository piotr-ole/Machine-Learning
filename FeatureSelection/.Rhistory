sapply(1:10,sqrt)
?power
?sqr
W <- square(c(-1,1))
?square
sum(sapply(1:25,function(i) {i^2})
stop
sum(sapply(1:2,function(i) {i*i}))
sum(sapply(1:25,function(i) {i*i}))
data(cars)
class(cars)
nrows(cars)
ncol(cars)
nrow(cars)
names(cars)
cars([,2])
cars[,2]
mean(cars[,2])
?which
which(LETTERS == "R")
which(cars[,2] == 85)
dir.create("Mice")
W <- square(10)
W <- square(c(-1,1))
plot(x,y)
source('~/.active-rstudio-document', echo=TRUE)
fit <- lm(y ~ x)
?list
pts <- list(x = cars[,1], y = cars[,2])
head(pts)
pts
typeof(pts)
class(pts)
plot(y ~ x)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
cars
head(cars)
data("CO2")
head(CO2)
summary(CO2)
remmove(list = ls())
remove(list = ls())
data("PlantGrowth")
head(PlantGrowth)
summary(PlantGrowth)
plot(PlantGrowth$weight ~ PlantGrowth$group)
plot(PlantGrowth$weight ~ PlantGrowth$group, xlab = "Control Group", ylab = "Plant Weight")
mean(PlantGrowth, which(PlantGrowth$group == "ctrl"))
?mean
library(swirl)
swirl()
bye()
head(dat) #In R Studio use View(dat)
filename <- "femaleMiceWeights.csv"
dat <- read.csv(filename)
head(dat) #In R Studio use View(dat)
dTat
dat
rm(list - ls())
rm(list = ls())
n <-  10
p <-  0.5
k <-  0:n
k
?dbinom
dbinom(k,n,p)
pk <- dbinom(k,n,p)
plot(k, pk)
plot(k, pk, type = 'h')
curve(pk)
dgeom(k,p)
p2 <- dgeom(k,p)
plot(p2)
#Zadanie 11
p_x_1 <- dgeom(0, 0.1)
?dgeom
p_x_less_11 <- pgeom(10, 0.1)
p_x_less_11 <- 1 - pgeom(10, 0.1)
p_k = (choose(M, k) * choose(N - M, sample_size - k)) / choose(N, sample_size)
#Zadanie 12
N <- 200 #sztuk w partii
M <- 5  #sztuk wadliwych
sample_size <- 10
#Jaka jest szansa, ze zadna nie jest wadliwa
k = 0
p_k = (choose(M, k) * choose(N - M, sample_size - k)) / choose(N, sample_size)
p_k
?dhyper
p_k_2 = dhyper(k, M, N - M, sample_size)
p_k_2
# X - liczba klientow w ciagu 1 godziny
dpois(0, lambd)
lambd = 4
# X - liczba klientow w ciagu 1 godziny
dpois(0, lambd)
p_x_0 = 1 - pexp(1,4)
p_x_0
pexp(0.5, 4)
1 - pexp(lambda, 1000)
lambda = 0.0001
1 - pexp(lambda, 1000)
1 - pexp(lambda, 10000)
1 - pexp(lambda, 30000)
?quantile
curve(pexp(lambda, x))
?pexp
1 - pexp(lambda, 1000)
1 - pexp(1000, lambda)
1 - pexp(10000, lambda)
1 - pexp(lambda, 30000)
1 - pexp(30000, lambda)
## Zadanie 15
x = dunif(1000, min = 0, max = 1)
x
dunif
?dunif
sample
?sample
## Zadanie 15
x = runif(1000, min = 0, max = 1)
y = runif(1000, min = 0, max = 1)
## Zadanie 15
n = 1000
x = runif(n, min = 0, max = 1)
y = runif(n, min = 0, max = 1)
count = 0
for (i in 0:length(x)) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
for (i in 1:length(x)) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
count / n
## Zadanie 15
n = 10000
x = runif(n, min = 0, max = 1)
y = runif(n, min = 0, max = 1)
count = 0
for (i in 1:length(x)) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
count / n
?runif
?integrate
# podpunkt a
fun <- function(x){
return(x*x)
}
integrate(fun, 0, 1)
x = seq(-1,1, 0.05)
y = x
plot(x, x*x)
plot(x, 1 - x*x, add = TRUE)
?plot
curve(x^2)
curve(x^2, xlim = c(-1,1))
curve(1- x^2, add =TRUE)
# podpunkt a inaczej
sum(y < x^2)
n = 10000
x = runif(n, min = 0, max = 1)
y = runif(n, min = 0, max = 1)
# podpunkt a inaczej
sum(y < x^2)
# podpunkt a inaczej
sum(y < x^2) / n
for (i in 1:length(x)) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
count / n
for (i in 1:n) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
count / n
count = 0
for (i in 1:n) {
if (y[i] < x[i] * x[i])
{
count = count + 1
}
}
count / n
sum(y > x^2 & y < (1 - x^2)) / n
curve(x^2, xlim = c(-1,1), col = "red")
curve(1-x^2, col = "blue", add = TRUE)
# podpunkt b
n = 10000
x = runif(n, min = -1, max = 1)
y = runif(n, min = 0, max = 1)
sum(y > x^2 & y < (1 - x^2)) / n
curve(x^2, xlim = c(-1,1), col = "red")
curve(1-x^2, col = "blue", add = TRUE)
# podpunkt b
n = 100000
x = runif(n, min = -100, max = 100)
y = runif(n, min = -100, max = 100)
sum(y > x^2 & y < (1 - x^2)) / n
x = runif(n, min = -10, max = 10)
y = runif(n, min = -10, max = 10)
sum(y > x^2 & y < (1 - x^2)) / n
y = runif(n, min = -1, max = 1)
sum(y > x^2 & y < (1 - x^2)) / n
# podpunkt b
n = 100000
x = runif(n, min = -sqrt(2)/2, max = sqrt(2)/2)
y = runif(n, min = 0, max = 1)
sum(y > x^2 & y < (1 - x^2)) / n
curve(x^2, xlim = c(-1,1), col = "red")
curve(1-x^2, col = "blue", add = TRUE)
sqrt(2) *sum(y > x^2 & y < (1 - x^2)) / n
# podpunkt b
n = 100000
# podpunkt b
n = 100000
x = runif(n, min = -sqrt(2)/2, max = sqrt(2)/2)
y = runif(n, min = 0, max = 1)
sqrt(2) *sum(y > x^2 & y < (1 - x^2)) / n
x = runif(n, min =  0, max = 1)
y = runif(n, min =  0, max = 1)
2 *sum(y > x^2 & y < (1 - x^2)) / n
# podpunkt b
n = 100000
x = runif(n, min = -sqrt(2)/2, max = sqrt(2)/2)
y = runif(n, min = 0, max = 1)
sqrt(2) *sum(y > x^2 & y < (1 - x^2)) / n
x = runif(n, min =  0, max = 1)
y = runif(n, min =  0, max = 1)
2 *sum(y > x^2 & y < (1 - x^2)) / n
setwd("C:/Users/Asus/Documents/Studia IAD/ZMUM/Machine-Learning/FeatureSelection")
# Projekt 2
# libraries & sources
library(caret)
library(e1071)
library(klaR)
library(mlbench)
library(Hmisc)
library(randomForest)
library(devtools)
library(bounceR)
library(xgboost)
library(FSelector)
rm(list = ls())
source("functions.R")
# Wczytanie danych
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
glm.fit <- glm(class ~. , data = cbind(dat, class = dat_labels))
varImp(glm.fit)
var_importance <- varImp(glm.fit)
var_names <- rownames(var_importance)
var_importance <- var_importance[order(var_importance, decreasing = TRUE)]
var_importance <- varImp(glm.fit)
var_names <- rownames(var_importance)
var_importance <- var_importance[order(var_importance, decreasing = TRUE)]
order(var_importance, decreasing = TRUE)
var_importance <- var_importance$Overall[order(var_importance$Overall, decreasing = TRUE)]
var_importance <- varImp(glm.fit)
var_names <- rownames(var_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
var_importance <- varImp(glm.fit)
var_names <- rownames(var_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
var_importance <- var_importance$Overall[ord]
vars_names <- vars_names[ord]
var_importance <- varImp(glm.fit)
var_names <- rownames(var_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
var_importance <- var_importance$Overall[ord]
var_names <- vars_names[ord]
var_names <- var_names[ord]
var_names[1:10]
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_xgb = numeric(cv_num)
acc = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
model.xgb <- xgboost(data = data.matrix(train[, vars_names[1:j]]), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.3)
pred_xgb <-  predict(model.xgb, newdata = data.matrix(test[, vars_names[1:j]]))
predicted_classes_xgb <- ifelse( pred_xgb > 0.5, 1, 0)
bcs_xgb[i + 1] <- balanced_acc(predicted_classes_xgb, test_lab)
t <- table(predicted_classes_xgb, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_xgb)))
}
cv_num <- 5
rep <- 5
nsize <- nrow(dat) / cv_num
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_xgb = numeric(cv_num)
acc = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
model.xgb <- xgboost(data = data.matrix(train[, vars_names[1:j]]), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.3)
pred_xgb <-  predict(model.xgb, newdata = data.matrix(test[, vars_names[1:j]]))
predicted_classes_xgb <- ifelse( pred_xgb > 0.5, 1, 0)
bcs_xgb[i + 1] <- balanced_acc(predicted_classes_xgb, test_lab)
t <- table(predicted_classes_xgb, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_xgb)))
}
vars_importance <- varImp(glm.fit)
vars_names <- rownames(var_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
vars_importance <- vars_importance$Overall[ord]
vars_names <- vars_names[ord]
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_xgb = numeric(cv_num)
acc = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
model.xgb <- xgboost(data = data.matrix(train[, vars_names[1:j]]), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.3)
pred_xgb <-  predict(model.xgb, newdata = data.matrix(test[, vars_names[1:j]]))
predicted_classes_xgb <- ifelse( pred_xgb > 0.5, 1, 0)
bcs_xgb[i + 1] <- balanced_acc(predicted_classes_xgb, test_lab)
t <- table(predicted_classes_xgb, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_xgb)))
}
rm(list = ls())
source("functions.R")
# Wczytanie danych
dat <- read.table('./data/artificial_train.data')
dat_labels <- read.table('./data/artificial_train.labels')
dat_labels <- dat_labels$V1
dat_labels[dat_labels == -1] = 0
glm.fit <- glm(class ~. , data = cbind(dat, class = dat_labels))
vars_importance <- varImp(glm.fit)
vars_names <- rownames(var_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
vars_importance <- vars_importance$Overall[ord]
vars_names <- vars_names[ord]
vars_names <- rownames(vars_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
vars_importance <- varImp(glm.fit)
vars_names <- rownames(vars_importance)
ord <- order(var_importance$Overall, decreasing = TRUE)
ord <- order(vars_importance$Overall, decreasing = TRUE)
vars_importance <- vars_importance$Overall[ord]
vars_names <- vars_names[ord]
cv_num <- 5
nsize <- nrow(dat) / cv_num
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_xgb = numeric(cv_num)
acc = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
model.xgb <- xgboost(data = data.matrix(train[, vars_names[1:j]]), label = train_lab,
objective = 'binary:logistic', verbose = 0, nrounds = 10, eta = 0.3)
pred_xgb <-  predict(model.xgb, newdata = data.matrix(test[, vars_names[1:j]]))
predicted_classes_xgb <- ifelse( pred_xgb > 0.5, 1, 0)
bcs_xgb[i + 1] <- balanced_acc(predicted_classes_xgb, test_lab)
t <- table(predicted_classes_xgb, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_xgb)))
}
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_xgb = numeric(cv_num)
acc = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
glm.model <- glm(class ~. , data = data.frame(train[,vars_names[1:i]], class = train_lab))
pred <- predict(glm.model, newdata = test[, vars_names[1:i]], type = 'response')
classes_pred <- ifelse(pred > 0.5, 1, 0)
bcs_xgb[i + 1] <- balanced_acc(predicted_classes_xgb, test_lab)
t <- table(predicted_classes_xgb, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_xgb)))
}
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_glm = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
glm.model <- glm(class ~. , data = data.frame(train[,vars_names[1:i]], class = train_lab))
pred <- predict(glm.model, newdata = test[, vars_names[1:i]], type = 'response')
classes_pred <- ifelse(pred > 0.5, 1, 0)
bcs_glm[i + 1] <- balanced_acc(classes_pred, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_glm)))
}
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
glm.model <- glm(class ~. , data = data.frame(train[,vars_names[1:i]], class = train_lab))
pred <- predict(glm.model, newdata = test[, vars_names[1:i]], type = 'response')
for (j in seq(2,20)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_glm = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
glm.model <- glm(class ~. , data = data.frame(train[,vars_names[1:j]], class = train_lab))
pred <- predict(glm.model, newdata = test[, vars_names[1:j]], type = 'response')
classes_pred <- ifelse(pred > 0.5, 1, 0)
bcs_glm[i + 1] <- balanced_acc(classes_pred, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_glm)))
}
vars_importance <- varImp(glm.fit)
varImpPlot(vars_importance)
library(earth)
install.packages("earth")
library(earth)
marsModel <- earth(class ~. , data = cbind(dat, class = dat_labels)) # build model
ev <- evimp(marsModel)
env
ev
varnames <- rownames(ev)
var_names <- rownames(ev)
s
vars_names <- rownames(ev)
for (j in seq(2,10)) {
samp <- sample(nrow(dat), replace = FALSE)
dt <- dat[samp, ]
dt_labels <- dat_labels[samp]
bcs_glm = numeric(cv_num)
for (i in seq(0, cv_num - 1)) {
ind <- (nsize*i + 1):(nsize*(i+1))
train <- dt[-ind, ]
test <- dt[ind, ]
train_lab <- dt_labels[-ind]
test_lab <- dt_labels[ind]
glm.model <- glm(class ~. , data = data.frame(train[,vars_names[1:j]], class = train_lab))
pred <- predict(glm.model, newdata = test[, vars_names[1:j]], type = 'response')
classes_pred <- ifelse(pred > 0.5, 1, 0)
bcs_glm[i + 1] <- balanced_acc(classes_pred, test_lab)
}
print(paste0(j, ": Balanced accuraccy (xgb) after cross-val: ",mean(bcs_glm)))
}
marsModel <- earth(class ~. , data = cbind(dat[, vars_names], class = dat_labels))
ind <- sample(nrow(dat), 0.8 *nrow(dat))
train <- dat[ind , vars_names]
ind <- sample(nrow(dat), 0.8 *nrow(dat))
train <- dat[ind , vars_names]
test <- dat[-ind, vars_names]
marsModel <- earth(class ~. , data = cbind(train[, vars_names], class = dat_labels))
marsModel <- earth(class ~. , data = cbind(train, class = dat_labels[ind]))
predict(marsModel, newdata = test)
pred <- predict(marsModel, newdata = test)
classes_pred <- ifelse(pred > 0.5, 1 , 0)
balanced_acc(classes_pred, dat_labels[-ind])
